# This Repo contains info about training SSD+InceptionV2

### Report on Training times

1. **model_main.py:** It evaluates the model for every checkpoint it generated. Basically it generates checkpoint for every 1200k steps with batch size of 32.

Below data is based on 350k steps on COCO metrics.
	Total Evals: 292
```
	Train 98% + Val 2%			4 days approx.
	Train 70% + Val 30%			48 days approx. (~4 hours for each evaluation)
	Train 90% + Val 10%		  	16 days approx.
```

2. **train.py** & **eval.py** are two separate legacy files

 Below data is based on 350k steps on COCO metrics.
```	
	Train 70 %			~3 days 
	Val 30%				Evaluating for every 20k steps i;e total 18 evals for 350k steps-> 3-4 days

	Total Training			~6-7 days.
```

   Below data is based on 350k steps on Pascal & Open Images V2 detection metrics.
	
```
	Train 70 %			~3 days 
	Val 30%				Evaluating for every 20k steps i;e total 18 evals for 350k steps-> 2-3 days

	Total Training			~5-6 days.
```

3. **Customized model_main.py:** Since original model_main.py is generating checkpoint for every 1200k steps with batch size of 32 and also is unable to handle COCO metrics on VAL 30% dataset. We customized the code to save the checkpoints at desired steps, for ex: you can save the checkpoints for every 10k or 20k or n steps. The original model_main.py only preserves latest 5 checkpoints, the customized code can preserve your desired number of checkpoints, for example: you can save latest 40 checkpoints. After doing all these necessary changes we noticed COCO evaluation is going well on our machine.

To customize the code, please change the following code.

In **model_main.py** go to **main()**, check the **tf.estimator.RunConfig** api and add the following additional parameters to the api **save_checkpoints_steps = "your desired checkpoint number"**, **keep_checkpoint_max = "no of checkpoints you want to save"**.

**Before customizing the code**
```
#model_main.py

def main(unused_argv):
  flags.mark_flag_as_required('model_dir')
  flags.mark_flag_as_required('pipeline_config_path')
  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir) ##adding the parameters to this api.
.
.
.
```

**After customizing the code**

```
#model_main.py

def main(unused_argv):
  flags.mark_flag_as_required('model_dir')
  flags.mark_flag_as_required('pipeline_config_path')
  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, save_checkpoints_steps = "your desired checkpoint number", keep_checkpoint_max = "no of checkpoints you want to save") ## added the additional parameters
.
.
.
```

**Note:** For CDCA, we used save_checkpoints_steps = 20000, keep_checkpoint_max = 40, so the checkpoint is generated for every 20k steps and saves the latest 40 checkpoints.

**4. If customized model_main.py fails with coco evaluation(backup solution)**

As we noticed original model_main.py is failing with COCO evaluation on our machine, we customized the model_main.py to make it work.

What if customized model_main.py also fails with coco evaluation? We have a backup plan.

We have two options:
1. To use train.py & eval.py
2. To use customized model_main.py & eval.py

We avoid to use **train.py** since it is unable to handle bigger batch sizes. So we can choose option 2, here we have to modify server script **model_lib.py** of model_main.py. The model_main.py cannot skip the evalution after generating the checkpoints. So what we can do is we can evaluate the generated checkpoints only on one image to continue the training, we should not consider the evaluation results generated by model_main.py since it is giving the result only for 1 image.
To evaluate the checkpoints generated by model_main.py, you can use eval.py. The result generated by eval.py is the original result.

To Customize the **model_lib.py**, find **tf.estimator.EvalSpec** api and change the **step** parameter from None to 1.
You can find **model_lib.py** in **./models/research/object_detection/**

**Before customizing the code**
```
tf.estimator.EvalSpec(
            name=str(eval_spec_name),
            input_fn=eval_input_fn,
            steps=None, ##Need to be changed
            exporters=exporter))
```

**After customizing the code**
```
tf.estimator.EvalSpec(
            name=str(eval_spec_name),
            input_fn=eval_input_fn,
            steps=1, ##Parameter changed to 1
            exporters=exporter))
```
Now you can start using customized model_main.py & eval.py for training.

### Customizing the maximum box coordinate value

The default **maximum_normalized_coordinate** in the SSD framework is **1.1**. If your images in the dataset is bigger, for example image size is 3000x3000, there is a chance maximum box coordinate value would exceed default maximum_normalized_coordinate in the image. This would go out of the threshold value and causes error. To solve this issue you need to neither remove those images from training or you need to modify the default **maximum_normalized_coordinate** value. For CDCA, we modified the value to **1.7**.

In order to modify the **maximum_normalized_coordinate** to your desired value. Please change the below scripts.

1. models/research/object_detection/utils/shape_utils.py

Find **assert_box_normalized()** function in shape_utils.py and modify **maximum_normalized_coordinate** parameter to your desired value.

```
#shape_utils.py

#Before modifying 

def assert_box_normalized(boxes, maximum_normalized_coordinate=1.1): ## change maximum_normalized_coordinate parameter to your desired value, default is 1.1.

#After modifying

def assert_box_normalized(boxes, maximum_normalized_coordinate=1.7): ## changed maximum_normalized_coordinate parameter to 1.7 for CDCA.
```



2. models/research/object_detection/core/box_list_ops.py

Find **to_absolute_coordinates()** function in box_list_ops.py and modify **maximum_normalized_coordinate** parameter to your desired value.

```
#box_list_ops.py

#Before modifying

def to_absolute_coordinates(boxlist,
                            height,
                            width,
                            check_range=True,
                            maximum_normalized_coordinate=1.1, ## change maximum_normalized_coordinate parameter to your desired value, default is 1.1.
                            scope=None):

#After modifying 

def to_absolute_coordinates(boxlist,
                            height,
                            width,
                            check_range=True,
                            maximum_normalized_coordinate=1.7, ## changed maximum_normalized_coordinate parameter to 1.7 for CDCA.
                            scope=None):


```
Now you don't get any issues in training.

**Note:** We found large images in OID V4.


### Arranging the things for training

List of items needed for training the algorithm:
1. TFRecords for train and validation
2. label_map file
3. Configuration file 

Recommended folder format for training

```
    --SSD
	--train.py
	--eval.py
	--model_main.py
	--export_inference_graph.py
	--labels.txt
	+data
	  -label_map file
	  -train TFRecord file
	  -val TFRecord file
	+models
	  + model
	    -Configuration file 
	    +train
	    +eval
```

Please copy all the generated train and val tfrecords manually to **data** folder. 

### Creating labelmap

To generate label map use **label_map_generator.py** file.  In order to execute this file you need to have **labels.txt**, **labels.txt** should include all the list of objects you are going to train. Be careful while listing the objects in **labels.txt**. You can find **label_map_generator.py** & **labels.txt** in githuib repo **./CDCA/training/**. Please copy label_map_generator.py, labels.txt files to SSD folder.

For CDCA
```
#From ./SSD/

python label_map_generator.py --name "honda_custom_label.pbtxt" --path "./labels.txt"

```

### Script for training the model with train.py

```
#From ./SSD/

python train.py \
        --logtostderr \
        --train_dir=path/to/train_dir \
        --pipeline_config_path=pipeline_config.pbtxt

```

### Script for evaluating model with eval.py

```
#From ./SSD/

python eval.py \
        --logtostderr \
        --checkpoint_dir=path/to/checkpoint_dir \
        --eval_dir=path/to/eval_dir \
        --pipeline_config_path=pipeline_config.pbtxt
```

### Script for training with model_main.py

```
#From ./SSD/

python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

```

### Freezing the trained weights
After you are done with the training, you need to freeze the weights. Use the below script to freeze the weights.

```
#From ./SSD/

python export_inference_graph.py \
    --input_type image_tensor \
    --pipeline_config_path path/to/ssd_inception_v2.config \
    --trained_checkpoint_prefix path/to/model.ckpt \
    --output_directory path/to/exported_model_directory
```
